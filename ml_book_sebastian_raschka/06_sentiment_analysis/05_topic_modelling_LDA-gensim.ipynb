{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This picture's following will only grow as tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Candy. Need we say more? He is the main r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This amazing documentary gives us a glimpse in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well, sadly, I can't help but feeling a little...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That's right. A movie written, directed and pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  This picture's following will only grow as tim...          1\n",
       "1  John Candy. Need we say more? He is the main r...          0\n",
       "2  This amazing documentary gives us a glimpse in...          1\n",
       "3  Well, sadly, I can't help but feeling a little...          1\n",
       "4  That's right. A movie written, directed and pr...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing very common occuring words using basic preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(sent_ence):\n",
    "    sent_ence = sent_ence.split(' ')\n",
    "    sent_ence = [i for i in sent_ence if i not in stop]\n",
    "    return ' '.join(sent_ence)\n",
    "\n",
    "df.review = df.review.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This picture's following grow time goes by. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Candy. Need say more? He main reason see ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This amazing documentary gives us glimpse live...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well, sadly, I can't help feeling little bit d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That's right. A movie written, directed produc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  This picture's following grow time goes by. Be...          1\n",
       "1  John Candy. Need say more? He main reason see ...          0\n",
       "2  This amazing documentary gives us glimpse live...          1\n",
       "3  Well, sadly, I can't help feeling little bit d...          1\n",
       "4  That's right. A movie written, directed produc...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['review'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "This picture's following grow time goes by. Better best picture nominees 97 rewards repeated viewings. I've seen three times I know. Anderson compared great American directors (Altman, Scorcese, Tarantino) may influences chances are, films, he'll considered part short list himself.<br /><br />One last note: Julianne Moore's \"Amber Waves\" resonate memory long 90's movie characters faded. THE best performance year -in four categories.\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and vectorize the documents\n",
    "\n",
    "    Tokenize (split the documents into tokens).\n",
    "\n",
    "    Lemmatize the tokens.\n",
    "\n",
    "    Compute bigrams.\n",
    "\n",
    "    Compute a bag-of-words representation of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numeric tokens and tokens that are only a single character\n",
    "\n",
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 20426\n",
      "Number of documents: 50000\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 8\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.9698.\n",
      "[([(0.01765403, 'like'),\n",
      "   (0.013317758, 'bad'),\n",
      "   (0.012545579, 'this'),\n",
      "   (0.01196481, 'even'),\n",
      "   (0.009842883, 'good'),\n",
      "   (0.009719115, 'really'),\n",
      "   (0.008870149, 'make'),\n",
      "   (0.008292448, 'that'),\n",
      "   (0.007922411, 'would'),\n",
      "   (0.007807308, 'acting'),\n",
      "   (0.0075955866, 'thing'),\n",
      "   (0.007400202, 'plot'),\n",
      "   (0.007286482, 'there'),\n",
      "   (0.0066941604, 'and'),\n",
      "   (0.0066787475, 'could'),\n",
      "   (0.006493284, 'get'),\n",
      "   (0.0060703927, 'people'),\n",
      "   (0.005998627, 'time'),\n",
      "   (0.0059191193, 'look'),\n",
      "   (0.0058302013, 'ever')],\n",
      "  -1.1440975713414971),\n",
      " ([(0.014232025, 'show'),\n",
      "   (0.012577127, 'time'),\n",
      "   (0.011757986, 'good'),\n",
      "   (0.011099146, 'see'),\n",
      "   (0.010892286, 'great'),\n",
      "   (0.010610615, 'like'),\n",
      "   (0.010390559, 'this'),\n",
      "   (0.008534627, 'first'),\n",
      "   (0.007922086, 'really'),\n",
      "   (0.0073557813, 'year'),\n",
      "   (0.007038468, 'funny'),\n",
      "   (0.00684613, 'well'),\n",
      "   (0.0064623747, 'would'),\n",
      "   (0.006290001, 'watch'),\n",
      "   (0.0061408584, 'still'),\n",
      "   (0.006087929, 'comedy'),\n",
      "   (0.005983048, 'think'),\n",
      "   (0.0058847573, 'love'),\n",
      "   (0.005715406, 'character'),\n",
      "   (0.0054269084, 'get')],\n",
      "  -1.277005642889836),\n",
      " ([(0.015391418, 'character'),\n",
      "   (0.0145982485, 'story'),\n",
      "   (0.0098917885, 'life'),\n",
      "   (0.006330184, 'people'),\n",
      "   (0.006036627, 'many'),\n",
      "   (0.005824502, 'way'),\n",
      "   (0.0053869584, 'in'),\n",
      "   (0.0053451215, 'this'),\n",
      "   (0.0052591744, 'make'),\n",
      "   (0.0051367367, 'much'),\n",
      "   (0.0050611314, 'u'),\n",
      "   (0.004902075, 'time'),\n",
      "   (0.0045052883, 'real'),\n",
      "   (0.0044025653, 'would'),\n",
      "   (0.0043132612, 'work'),\n",
      "   (0.0042568366, 'well'),\n",
      "   (0.003892198, 'audience'),\n",
      "   (0.0038384052, 'point'),\n",
      "   (0.0038194205, 'world'),\n",
      "   (0.0036842057, 'book')],\n",
      "  -1.5077536795503872),\n",
      " ([(0.008686347, 'he'),\n",
      "   (0.008271036, 'get'),\n",
      "   (0.0065454496, 'man'),\n",
      "   (0.006272094, 'scene'),\n",
      "   (0.004987017, 'go'),\n",
      "   (0.0049201045, 'take'),\n",
      "   (0.0044064294, 'there'),\n",
      "   (0.0041837064, 'horror'),\n",
      "   (0.0040128184, 'two'),\n",
      "   (0.0036561717, 'in'),\n",
      "   (0.003611533, 'end'),\n",
      "   (0.0035133685, 'house'),\n",
      "   (0.0034670213, 'around'),\n",
      "   (0.0034621765, 'woman'),\n",
      "   (0.0034577765, 'back'),\n",
      "   (0.0033624587, 'turn'),\n",
      "   (0.0033098448, 'find'),\n",
      "   (0.0032264215, 'little'),\n",
      "   (0.0029869925, 'death'),\n",
      "   (0.002921486, 'dead')],\n",
      "  -1.7196412605924043),\n",
      " ([(0.016400168, 'performance'),\n",
      "   (0.013417409, 'role'),\n",
      "   (0.011374849, 'best'),\n",
      "   (0.011350871, 'actor'),\n",
      "   (0.011249227, 'cast'),\n",
      "   (0.009875154, 'great'),\n",
      "   (0.00927553, 'music'),\n",
      "   (0.009165111, 'play'),\n",
      "   (0.008955946, 'well'),\n",
      "   (0.006229268, 'also'),\n",
      "   (0.0060585467, 'director'),\n",
      "   (0.005835636, 'excellent'),\n",
      "   (0.0056142635, 'version'),\n",
      "   (0.005545094, 'song'),\n",
      "   (0.0054454408, 'star'),\n",
      "   (0.005420931, 'work'),\n",
      "   (0.004825575, 'screen'),\n",
      "   (0.004414607, 'scene'),\n",
      "   (0.0042617423, 'production'),\n",
      "   (0.004240492, 'john')],\n",
      "  -1.8559204466553878),\n",
      " ([(0.020201135, 'she'),\n",
      "   (0.019282073, 'love'),\n",
      "   (0.016341189, 'woman'),\n",
      "   (0.015485866, 'young'),\n",
      "   (0.014539112, 'her'),\n",
      "   (0.01321401, 'father'),\n",
      "   (0.012419997, 'family'),\n",
      "   (0.011081298, 'he'),\n",
      "   (0.011052004, 'wife'),\n",
      "   (0.010944198, 'mother'),\n",
      "   (0.010921013, 'life'),\n",
      "   (0.010685371, 'man'),\n",
      "   (0.010040884, 'son'),\n",
      "   (0.008851662, 'girl'),\n",
      "   (0.00811818, 'daughter'),\n",
      "   (0.0072749066, 'husband'),\n",
      "   (0.0066506243, 'sister'),\n",
      "   (0.005609438, 'him'),\n",
      "   (0.0055298153, 'friend'),\n",
      "   (0.0052854577, 'brother')],\n",
      "  -2.00840356281235),\n",
      " ([(0.012634632, 'war'),\n",
      "   (0.012143005, 'american'),\n",
      "   (0.0074589597, 'year'),\n",
      "   (0.006796969, 'world'),\n",
      "   (0.0055009285, 'black'),\n",
      "   (0.005096394, 'white'),\n",
      "   (0.0050113304, 'new'),\n",
      "   (0.004416036, 'country'),\n",
      "   (0.0042006224, 'city'),\n",
      "   (0.003878701, 'soldier'),\n",
      "   (0.0038174393, 'in'),\n",
      "   (0.003728169, 'space'),\n",
      "   (0.0036171402, 'earth'),\n",
      "   (0.0036093704, 'america'),\n",
      "   (0.003471505, 'men'),\n",
      "   (0.0032518355, 'german'),\n",
      "   (0.0031804952, 'history'),\n",
      "   (0.0031797676, 'fi'),\n",
      "   (0.003178305, 'british'),\n",
      "   (0.0031690027, 'sci')],\n",
      "  -2.781190074351927),\n",
      " ([(0.034164708, 'action'),\n",
      "   (0.015998371, 'fight'),\n",
      "   (0.012141102, 'vampire'),\n",
      "   (0.008409828, 'villain'),\n",
      "   (0.0071339416, 'bruce'),\n",
      "   (0.006989932, 'match'),\n",
      "   (0.00697233, 'fighting'),\n",
      "   (0.006818804, 'lee'),\n",
      "   (0.006737154, 'hero'),\n",
      "   (0.006206786, 'batman'),\n",
      "   (0.00613066, 'art'),\n",
      "   (0.005827219, 'bond'),\n",
      "   (0.005627719, 'martial'),\n",
      "   (0.0056228368, 'robot'),\n",
      "   (0.0055138594, 'comic'),\n",
      "   (0.0052660326, 'van'),\n",
      "   (0.0050852937, 'sequence'),\n",
      "   (0.004854779, 'tarzan'),\n",
      "   (0.004797793, 'fight_scene'),\n",
      "   (0.004778628, 'star')],\n",
      "  -3.4641648544525725)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on reading the five most important words for each topic, you may guess that the LDA identified the following topics:\n",
    "\n",
    "    # Random Category 1\n",
    "    Comedy movies\n",
    "    Biographical movies\n",
    "    Horror movies\n",
    "    # Random Category 2\n",
    "    Family movies\n",
    "    Historical movies\n",
    "    Action movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are familiar with the subject of the articles in this dataset, you can see that the topics below make a lot of sense. However, they are not without flaws. We can see that there is substantial overlap between some topics, others are hard to interpret, and most of them have at least some terms that seem out of place"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
